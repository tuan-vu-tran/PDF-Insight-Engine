!pip install -U langchain-text-splitters
!pip install -U langchain-community langchain-text-splitters langchain
!pip install pypdf
!pip install -U langchain-huggingface
!pip install -qU langchain-chroma
!pip install -qU langchain
!pip install -qU langchain-groq
!pip install sentence-transformers
!pip install -q gradio


# DAY 1
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader


loader = PyPDFLoader("../content/sample_data/assurance.pdf")
pdf = loader.load()

print(f"Document charg√© : {len(pdf)} pages trouv√©es.")


text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,      # Taille cible de chaque morceau
    chunk_overlap = 150,    # Chevauchement
    separators = ["\n\n", "\n", " ", ""] # L'ordre de priorit√© pour couper
)

chunks = text_splitter.split_documents(pdf)


print(f"Nombre total de chunks pour {len(pdf)} pages : {len(chunks)}")

lengths = [len(c.page_content) for c in chunks]
import statistics
print(f"Taille moyenne : {statistics.mean(lengths)} caract√®res")

# Voir √† quelle page appartient le chunk n¬∞10
print(f"Le chunk 10 vient de la page : {chunks[10].metadata['page']}")


# DAY 2
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

hf_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# Cr√©ation de la base de donn√©es physique
vectordb = Chroma.from_documents(
    documents=chunks,
    embedding=hf_embeddings,
    persist_directory="./chroma_db"
)


question = "Quelles sont les garanties de mon contrat ?"
question_embedding = hf_embeddings.embed_documents([question])

# Demande les 'k' meilleurs r√©sultats
# Chroma fait l'embedding de la question et le calcul de similarit√©
docs = vectordb.similarity_search(question, k=1)

print(f"Question : {question}")
print("---")
print(f"R√©ponse: {docs[0].page_content}")


# DAY 3
from langchain_groq import ChatGroq
import os


llm = ChatGroq(
    model_name="llama-3.1-8b-instant",
    temperature=0.2
)


from langchain_classic.chains import RetrievalQA

# Transformer la base Chroma en "chercheur" (retriever)
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

# Cr√©er la cha√Æne RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", # "stuff" veut dire : "donne tout le texte au LLM"
    retriever=retriever
)


response = qa_chain.invoke("Quelles sont les garanties de mon contrat ?")
print(response["result"])


# DAY 4


import gradio as gr
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationalRetrievalChain

# 1. Configurer la m√©moire
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)


# 2. Cr√©er la cha√Æne avec m√©moire
qa_chat = ConversationalRetrievalChain.from_llm(
    llm=llm, # LLM Groq
    retriever=retriever, # retriever Chroma
    memory=memory
)


# 3. Fonction pour l'interface Gradio
def chat_interactif(message, history):
    response = qa_chat.invoke({"question": message})
    return response["answer"]


# 4. Lancer l'interface
demo = gr.ChatInterface(
    fn=chat_interactif,
    title="üìö Mon Assistant PDF Intelligent",
    description="Posez des questions sur votre document, je m'en souviendrai !",
    examples=["Fais-moi un r√©sum√©", "Quels sont les points cl√©s ?"],
    theme="soft"
)


demo.launch(share=True) # share=True cr√©e un lien public de 72h !



